# Rust Visibility Optimization Notes

Status: **post polygon-intersection refactor** — re-profile recommended

## Context

The visibility system computes per-observer visibility polygons via angular-sweep raycasting, then uses those polygons for several scoring metrics:

- **Overall visibility**: Grid sample points inside the visibility polygon vs total grid points.
- **DZ hideability**: Whether the visibility polygon overlaps the opponent's expanded deployment zone (polygon-polygon intersection test).
- **Objective hidability**: Whether objective sample points fall inside the visibility polygon (PIP via Z-sorted binary search).

The angular-sweep raycasting (`compute_visibility_polygon`) casts rays toward segment endpoints (±epsilon), finds the nearest intersection per ray, and forms the visibility polygon. The hot loop is O(R × S) where R = rays, S = segments, reduced to O(R × k) by angular bucketing.

Real-world workloads: ~100-200 unique endpoints → ~300-600 rays, ~40-80 terrain segments + 4 table boundary segments.

## Benchmark Setup (IMPORTANT)

The benchmark fixtures in `benches/generate_bench.rs` were originally using **2" tall crates**, which are below the 4" `min_blocking_height` threshold. This meant the "visibility" benchmarks weren't actually testing the intersection loop at all — every observer hit the fast path (0 segments). The fixtures were fixed to use **5" tall crates** as part of this work.

Baseline numbers (with corrected 5" crates, before any optimization):
- `visibility_50`: 872 ms
- `visibility_100`: 3.80 s
- `mission_hna`: 5.70 s

### Benchmark fixtures (28-case pairwise covering array)

The benchmarks use a pairwise (all-pairs) covering array to exercise all 2-way parameter interactions with minimal test cases. Generated by `benches/generate_fixtures.py`.

**Parameters (9):**
- **Table size** (3): 44×30 (incursion), 60×44 (strike), 90×44 (onslaught)
- **Symmetry** (2): off, on (forced off when mission=none)
- **Mission** (7): none, Hammer & Anvil, Dawn of War, Tipping Point, Sweeping Engagement, Crucible of Battle, Search & Destroy
- **Terrain** (2): crates-only (5" tall), WTC mixed (crates + ruins + walls)
- **Steps** (4): 10, 20, 50, 100
- **Feature gap** (2): 0 or 5.2" (`min_feature_gap_inches`)
- **Edge gap** (2): 0 or 5.2" (`min_edge_gap_inches`)
- **All-feature gap** (2): 0 or 3" (`min_all_feature_gap_inches`)
- **All-edge gap** (2): 0 or 3" (`min_all_edge_gap_inches`)

All 28 cases use seed=42, visibility enabled, scoring_targets matching UI defaults (overall=30%/w1, dz_hide=70%/w5, obj_hide=50%/w5). Mission cases include precomputed `expanded_polygons` (6" DZ expansion via shapely), so the DZ hideability polygon-polygon intersection path is actually exercised.

Gap suffix `_gFEAA` encodes gap params as 4-bit flags: F=feature_gap, E=edge_gap, A=all_feature_gap, A=all_edge_gap (0=off, 1=on: 5.2" for F/E, 3" for A/A).

| # | Benchmark | Table | Sym | Mission | Terrain | Steps | Gaps |
|---|-----------|-------|-----|---------|---------|-------|------|
| 01 | `44x30_crates_none_nosym_10_g0000` | 44×30 | off | none | crates | 10 | 0000 |
| 02 | `60x44_wtc_none_nosym_20_g1010` | 60×44 | off | none | WTC | 20 | 1010 |
| 03 | `90x44_crates_none_nosym_50_g1101` | 90×44 | off | none | crates | 50 | 1101 |
| 04 | `44x30_wtc_none_nosym_100_g0111` | 44×30 | off | none | WTC | 100 | 0111 |
| 05 | `60x44_crates_HnA_nosym_10_g1001` | 60×44 | off | HnA | crates | 10 | 1001 |
| 06 | `90x44_wtc_HnA_sym_20_g0100` | 90×44 | on | HnA | WTC | 20 | 0100 |
| 07 | `44x30_wtc_HnA_nosym_50_g0110` | 44×30 | off | HnA | WTC | 50 | 0110 |
| 08 | `60x44_crates_HnA_sym_100_g1011` | 60×44 | on | HnA | crates | 100 | 1011 |
| 09 | `90x44_wtc_DoW_nosym_10_g0110` | 90×44 | off | DoW | WTC | 10 | 0110 |
| 10 | `44x30_crates_DoW_sym_20_g1001` | 44×30 | on | DoW | crates | 20 | 1001 |
| 11 | `60x44_crates_DoW_nosym_50_g0011` | 60×44 | off | DoW | crates | 50 | 0011 |
| 12 | `90x44_wtc_DoW_sym_100_g1100` | 90×44 | on | DoW | WTC | 100 | 1100 |
| 13 | `44x30_wtc_TipPt_sym_10_g1010` | 44×30 | on | TipPt | WTC | 10 | 1010 |
| 14 | `60x44_crates_TipPt_nosym_20_g0101` | 60×44 | off | TipPt | crates | 20 | 0101 |
| 15 | `90x44_crates_TipPt_sym_50_g1001` | 90×44 | on | TipPt | crates | 50 | 1001 |
| 16 | `44x30_wtc_TipPt_nosym_100_g0110` | 44×30 | off | TipPt | WTC | 100 | 0110 |
| 17 | `60x44_wtc_SwpEng_sym_10_g0101` | 60×44 | on | SwpEng | WTC | 10 | 0101 |
| 18 | `90x44_crates_SwpEng_nosym_20_g1010` | 90×44 | off | SwpEng | crates | 20 | 1010 |
| 19 | `44x30_crates_SwpEng_sym_50_g1101` | 44×30 | on | SwpEng | crates | 50 | 1101 |
| 20 | `60x44_wtc_SwpEng_nosym_100_g0010` | 60×44 | off | SwpEng | WTC | 100 | 0010 |
| 21 | `90x44_crates_Crucible_sym_10_g1000` | 90×44 | on | Crucible | crates | 10 | 1000 |
| 22 | `44x30_wtc_Crucible_nosym_20_g0111` | 44×30 | off | Crucible | WTC | 20 | 0111 |
| 23 | `60x44_wtc_Crucible_sym_50_g0110` | 60×44 | on | Crucible | WTC | 50 | 0110 |
| 24 | `90x44_crates_Crucible_nosym_100_g1001` | 90×44 | off | Crucible | crates | 100 | 1001 |
| 25 | `44x30_crates_SnD_nosym_10_g0111` | 44×30 | off | SnD | crates | 10 | 0111 |
| 26 | `60x44_wtc_SnD_sym_20_g1000` | 60×44 | on | SnD | WTC | 20 | 1000 |
| 27 | `90x44_wtc_SnD_nosym_50_g0010` | 90×44 | off | SnD | WTC | 50 | 0010 |
| 28 | `44x30_crates_SnD_sym_100_g1101` | 44×30 | on | SnD | crates | 100 | 1101 |

Criterion config: `sample_size(10)`, `warm_up_time(500ms)`, `measurement_time(3s)` — total suite runtime ~5 minutes.

For quick iteration, filter to specific cases: `cargo bench -- 60x44_crates_HnA` or `cargo bench -- _100` (all 100-step cases).

### Baseline results (February 2026, 9-parameter suite with scoring_targets + gaps)

| # | Benchmark | Mean |
|---|-----------|------|
| 01 | `44x30_crates_none_nosym_10_g0000` | 10.2 ms |
| 02 | `60x44_wtc_none_nosym_20_g1010` | 30.5 ms |
| 03 | `90x44_crates_none_nosym_50_g1101` | 164 ms |
| 04 | `44x30_wtc_none_nosym_100_g0111` | 112 ms |
| 05 | `60x44_crates_HnA_nosym_10_g1001` | 30.1 ms |
| 06 | `90x44_wtc_HnA_sym_20_g0100` | 220 ms |
| 07 | `44x30_wtc_HnA_nosym_50_g0110` | 161 ms |
| 08 | `60x44_crates_HnA_sym_100_g1011` | 678 ms |
| 09 | `90x44_wtc_DoW_nosym_10_g0110` | 37.2 ms |
| 10 | `44x30_crates_DoW_sym_20_g1001` | 38.7 ms |
| 11 | `60x44_crates_DoW_nosym_50_g0011` | 203 ms |
| 12 | `90x44_wtc_DoW_sym_100_g1100` | **1.86 s** |
| 13 | `44x30_wtc_TipPt_sym_10_g1010` | 30.2 ms |
| 14 | `60x44_crates_TipPt_nosym_20_g0101` | 87.1 ms |
| 15 | `90x44_crates_TipPt_sym_50_g1001` | 604 ms |
| 16 | `44x30_wtc_TipPt_nosym_100_g0110` | 373 ms |
| 17 | `60x44_wtc_SwpEng_sym_10_g0101` | 42.3 ms |
| 18 | `90x44_crates_SwpEng_nosym_20_g1010` | 83.2 ms |
| 19 | `44x30_crates_SwpEng_sym_50_g1101` | 111 ms |
| 20 | `60x44_wtc_SwpEng_nosym_100_g0010` | 848 ms |
| 21 | `90x44_crates_Crucible_sym_10_g1000` | 50.0 ms |
| 22 | `44x30_wtc_Crucible_nosym_20_g0111` | 39.3 ms |
| 23 | `60x44_wtc_Crucible_sym_50_g0110` | 253 ms |
| 24 | `90x44_crates_Crucible_nosym_100_g1001` | 773 ms |
| 25 | `44x30_crates_SnD_nosym_10_g0111` | 15.1 ms |
| 26 | `60x44_wtc_SnD_sym_20_g1000` | 100 ms |
| 27 | `90x44_wtc_SnD_nosym_50_g0010` | 476 ms |
| 28 | `44x30_crates_SnD_sym_100_g1101` | 207 ms |

**Observations:**
- Symmetry is expensive: sym cases are ~2-3x slower than nosym at same step count (doubled features → doubled visibility work).
- The 100-step sym cases on large tables (12: 90×44 DoW sym 100) are the heaviest at ~1.9s, dominated by visibility scoring with many features.
- Gap enforcement has modest impact: comparing old no-gap results to new gap-enabled results, gap checking adds ~5-15% overhead on mutation-heavy workloads.
- Scoring targets (multi-metric optimization) makes 100-step cases noticeably faster than old no-target results — the engine converges faster with explicit targets, spending fewer steps in expensive visibility phases.
- 10-step cases are fast (~10-50ms) regardless of other parameters — useful for quick regression checks.

## Committed Optimizations

### 1. Angular bucketing (commit `47e451c`)
Partition segments by angular extent into 64 buckets. Each ray only tests segments in its bucket, reducing work from O(R×S) to O(R×k) where k ≈ average segments per bucket.

- visibility_50: 872ms → 688ms (**-21%**)
- visibility_100: 3.80s → 2.19s (**-42%**)
- mission_hna: 5.70s → 4.76s (**-16%**)

### 2. ~~Trig reduction in ray generation~~ (commit `43c1bfa`, **reverted**)
Originally replaced 6 cos/sin calls per endpoint with 1 sqrt + small-angle rotation matrix for ±epsilon rays. **Reverted for FP parity**: the rotation matrix `ndx * cos_eps ± ndz * sin_eps` produces different IEEE 754 results than Python's `cos(angle ± eps)` / `sin(angle ± eps)`, causing visibility polygon vertex differences that propagated into DZ PIP boundary tests. The code now uses `cos(angle ± eps)` / `sin(angle ± eps)` (4 trig calls per endpoint) to match Python exactly.

### 3. Inlined intersection + removed polygon buffer (commit `dfd7351`)
Inline `ray_segment_intersection` in the hot loop to enable early exit when `t >= min_t` (skips u computation). Eliminate intermediate `polygon` Vec, write directly to result.

- visibility_50: 688ms → 583ms (**-15%**)
- visibility_100: 2.19s → 1.84s (**-16%**)
- mission_hna: 4.76s → 4.88s (~same)

*Note: numbers updated to reflect opt #2 revert (baseline is post-bucketing, not post-trig-reduction).*

### 4. Rayon parallel observer loop (commit `3f719bc`)
Parallelize the observer loop in `compute_layout_visibility` using `rayon::par_iter().fold().reduce()`. Each thread gets a `Box<ThreadAccum>` with its own working buffers and accumulators. Merge via sum for ratios/counts and OR for boolean seen arrays.

Note: rayon parallelism changes the order observers are processed, but the final result is mathematically identical (addition is commutative). Parity with Python engine is maintained.

### 5–6. ~~Z-sorted binary search for DZ PIP~~ (**superseded by architecture change**)
Optimizations #5 and #6 introduced Z-sorted binary search (`DzSortedSamples`, `fraction_of_dz_visible_zsorted`, `pip_zsorted_update_seen`) for point-in-polygon testing against DZ sample grids. These were the biggest wins on mission workloads (~49% and ~41% respectively on `mission_hna`).

**These optimizations are now largely superseded.** The DZ visibility system was refactored (commit `703487f`) to replace PIP-based grid sampling with polygon-polygon intersection (`polygons_overlap`). The old `dz_visibility` and `dz_to_dz_visibility` metrics (which tested hundreds of DZ sample points against each observer's visibility polygon) were replaced by a single `dz_hideability` metric that tests whether the visibility polygon overlaps the opponent's expanded DZ polygon — a direct geometric check with no sampling.

Z-sorted PIP code still exists in the codebase and is used for **objective hidability** (`pip_zsorted_update_seen` for objective sample points), but the dominant DZ paths no longer use it.

### 7. DZ visibility: polygon-polygon intersection (commit `703487f`)
Replace expensive per-observer PIP sampling for DZ metrics with direct polygon-polygon intersection testing. DZ polygons are expanded by a fixed radius (using shapely in Python, passed to Rust as precomputed `expanded_polygons`). Each observer tests whether its visibility polygon overlaps the opponent's expanded DZ — a single `polygons_overlap()` call per DZ pair, using edge-edge intersection + vertex containment.

This eliminates the entire DZ sample grid, Z-sorted binary search for DZ PIP, and complex batch encoding logic. The new `dz_hideability` metric is both faster and more precise (exact geometric intersection vs grid approximation).

- visibility_50: ~201ms → ~191ms (no DZs — within noise)
- visibility_100: ~719ms → ~733ms (no DZs — within noise)
- **mission_hna: ~629ms → ~364ms (-42%)**
- **mission_ruins: ~371ms → ~192ms (-48%)**

### Cumulative improvement (all optimizations)
| Benchmark | Original | Current | Total improvement |
|---|---|---|---|
| visibility_50 | 872 ms | **~191 ms** | **-78%** |
| visibility_100 | 3.80 s | **~733 ms** | **-81%** |
| mission_hna | 5.70 s | **~364 ms** | **-94%** |
| mission_ruins | n/a | **~192 ms** | n/a |

*Measured February 2026.*

### FP parity reversions (post-optimization correctness fixes)

Three micro-optimizations were reverted because they produced different IEEE 754 floating-point results than the Python engine, breaking bit-identical parity:

1. **Trig reduction (opt #2)**: Rotation matrix for ±eps rays → reverted to `cos(angle ± eps)` / `sin(angle ± eps)`. Cost: +4 trig calls per endpoint.
2. **Precomputed inv_dz (opt #5 micro-opt)**: `1.0 / (zj - zi)` then multiply → reverted to direct division `/ (zj - zi)`. Cost: 1 division per edge per Z-range point instead of 1 multiply.
3. **Ray normalization inv_len**: `dx * (1.0 / len)` → reverted to `dx / len`. Cost: negligible (1 division vs 1 multiply, once per endpoint).

**Lesson learned**: Any arithmetic expression that produces values used in boundary tests (PIP edge crossings, visibility polygon vertices fed to DZ PIP) must use the exact same FP expression order as Python. `a * b * (1/c)` ≠ `a * b / c` and `cos(atan2(y,x) ± eps)` ≠ rotation matrix at IEEE 754 level.

## Attempted But Abandoned

### Segment-first loop reordering
Flip ray-outer/segment-inner to segment-outer/ray-inner with a `min_t[]` array (like `batch_point_in_polygon` does for PIP). The idea was to keep segment data in registers while scanning rays linearly.

**Result**: No clear improvement (±3% noise). The array-based `min_t` prevents register allocation of the per-ray minimum, and LLVM already does a good job with the original loop.

### Precomputed segment data
Precompute `(sx, sz, d_x1, d_z1, num_t)` per segment before the ray loop to avoid redundant arithmetic.

**Result**: Made things **worse** (~+15%). The 40-byte precomputed tuples increased the inner loop's memory footprint. The original 32-byte segment tuples with recomputed arithmetic were faster due to better cache behavior.

### AABB pre-filter on batch_point_in_polygon
Compute the bounding box of the visibility polygon and build a candidates list of points inside the AABB. Only run the edge-crossing loop on candidates. The idea was to skip 60-80% of DZ points when the vis polygon is small.

**Result**: No improvement on mission_hna. With sparse terrain, each observer's visibility polygon covers nearly the entire table, so the AABB filter skips almost nothing while adding overhead. The AABB filter has fundamentally wrong assumptions about the problem: in visibility analysis, most observers see most of the table.

### Pseudoangle (replacing atan2)
Replace atan2 with a cheap pseudoangle function `p = dx / (|dx| + |dz|)` that maps monotonically to [0, 4). Eliminates all atan2 calls (ray generation + bucket assignment).

**Result**: Mixed. Helped visibility_50 (-19%) and visibility_100 (-14%), but mission_hna regressed (+12%). The pseudoangle maps non-linearly to real angles, causing uneven bucket distribution. Buckets near the cardinal axes become wider (more segments), creating load imbalance. **Worth retrying with a clean machine.**

## Profiling Results

### Historical — pre-polygon-intersection (no longer current)

The following profiles were taken before the DZ visibility refactor and are retained for reference. The `dz_vis` and `cross_dz` phases no longer exist in their old form — they have been replaced by polygon-polygon intersection.

<details>
<summary>Post-Z-sorted-dz_vis, pre-Z-sorted-cross_dz (mission_hna)</summary>

Late-game steps (~760 observers, ~20 segments):

| Phase | Thread-ms | % of observer loop |
|---|---|---|
| `compute_visibility_polygon` | 30-50 | **~15-18%** |
| `dz_vis` (Z-sorted) | 20-30 | **~9-12%** |
| `cross_dz` (batch PIP) | 100-130 | **~45-50%** |
| `obj_hide` (batch PIP) | 55-80 | **~25-30%** |

</details>

<details>
<summary>Post-rayon, pre-Z-sorted (mission_hna)</summary>

| Phase | Thread-ms | % of observer loop |
|---|---|---|
| `get_observer_segments` | 0.2 | ~0.04% |
| `compute_visibility_polygon` | 35-48 | **~8%** |
| `dz_vis` (fraction_of_dz_visible_batch) | 270-330 | **~60%** |
| `cross_dz` (batch_point_in_polygon) | 120-148 | **~27%** |
| `obj_hide` (batch_point_in_polygon) | 30-47 | **~7%** |

</details>

### Current state: needs re-profiling

The polygon-polygon intersection refactor significantly changed the observer loop structure. The old DZ PIP phases (`dz_vis`, `cross_dz`) are replaced by `polygons_overlap` calls. Fresh profiling is needed to identify the new bottleneck distribution. Expected phases:

- `compute_visibility_polygon` — angular sweep raycasting (likely dominant now)
- `dz_hideability` — polygon-polygon intersection per observer per opponent DZ
- `obj_hidability` — Z-sorted PIP for objective sample points (still uses old approach)
- `overall_pip` — grid point PIP for overall visibility percentage

## Future Optimization Ideas (Not Yet Tried)

Organized by expected impact. Updated after polygon-intersection refactor.

### Tier 1: Polygon-polygon intersection optimization

The new `polygons_overlap()` function (in `collision.rs`) uses edge-edge intersection + vertex containment. It tests all edges of polygon A against all edges of polygon B, plus vertex-in-polygon checks.

#### 1a. AABB early-exit for polygons_overlap
Compute bounding boxes of both polygons; if AABBs don't overlap, return false immediately. With sparse terrain on a large table, many observer visibility polygons won't overlap distant DZ polygons at all. This is cheap (O(V) to compute AABB) and could skip the O(E_A × E_B) edge tests entirely for non-overlapping cases.

#### 1b. Spatial acceleration for edge-edge tests
If expanded DZ polygons have many edges (shapely buffer can produce ~20-50 points per polygon), the O(E_vis × E_dz) edge test becomes significant. Precomputing a spatial index (e.g., grid-based or segment tree) on the static DZ polygon edges could reduce per-observer work.

### Tier 2: Raycasting refinements (dominant for non-mission workloads)

With DZ PIP eliminated as a bottleneck, raycasting (`compute_visibility_polygon`) is likely the dominant cost again for mission workloads as well as non-mission. These ideas target the angular sweep hot loop.

#### 2a. Pseudoangle hybrid
Use pseudoangle `p = dx / (|dx| + |dz|)` for ray sorting (avoids atan2 in ray generation) but keep atan2 for bucket assignment (uniform bucket distribution).

**Note**: Since the trig reduction (opt #2) was reverted, ray generation now does `atan2 + 4×cos/sin + 1×sqrt + 2×division` per endpoint. Pseudoangle could replace the atan2 for sort-key computation (but the 4 cos/sin calls for ±eps rays must stay for FP parity).

#### 2b. Faster endpoint deduplication
The `HashSet<(u64, u64)>` for endpoint dedup uses SipHash (DoS-resistant but slow). Options:
- Replace with `FxHashSet` from `rustc-hash` crate (fast hash, would add a dependency)
- Implement a simple custom hasher (XOR + multiply, no dependency)
- Skip dedup entirely (costs ~80% more rays but avoids hashing overhead — net effect unclear)

#### 2c. SIMD intersection
Manually vectorize the ray-segment intersection using `std::arch` SIMD intrinsics. Complex to implement and maintain. Low priority unless profiling shows raycasting > 50%.

### Tier 3: Objective hidability PIP (still uses Z-sorted binary search)

Objective hidability still uses `pip_zsorted_update_seen` to test objective sample points against visibility polygons. With the DZ PIP paths removed, this is the only remaining PIP-heavy path.

#### 3a. Slab decomposition for objective PIP
Preprocess the visibility polygon into horizontal slabs (O(E log E) setup). Per point, binary-search to find its slab in O(log E), test against 1-2 edges. Turns PIP from O(E) per point to O(log E) per point.

With typical objective circles (~40-80 sample points per objective, 5 objectives), this is a smaller workload than the old DZ PIP. May not be worth the complexity unless profiling shows obj_hide is a significant fraction.

### Tier 4: Collision / mutation path (affects all workloads)

#### 4a. Redundant OBB computation in is_valid_placement
`get_world_obbs()` and `get_tall_world_obbs()` are called multiple times for the *same* features during a single `is_valid_placement()` call. Computing OBBs once per feature and filtering by height would eliminate redundant trig and allocation.

#### 4b. Mirror feature cloning in hot paths
Mirror features for rotationally symmetric layouts are cloned into a fresh Vec on every call. Could precompute mirrors once per step, or use lazy iterator adapters.

### Tier 5: Tempering / allocation overhead

#### 5a. Pre-allocate sub_undos
`Vec::with_capacity(num_mutations)` allocated fresh for every step of every replica. Could reuse with `clear()`.

#### 5b. Layout cloning on swap and best-tracking
Full `TerrainLayout` cloned on replica swaps. Could use COW semantics.

### Tier 6: Algorithmic / architectural (high complexity, speculative)

#### 6a. Incremental visibility
Only recompute observers affected by a mutation. Extremely complex, especially with DZ/objective accumulation. Risk of subtle correctness bugs.

### Recommended next steps

1. **Re-profile mission_hna and mission_ruins** to see the new bottleneck distribution post-polygon-intersection. Expected: `compute_visibility_polygon` is now dominant.
2. **Try 1a (AABB early-exit for polygons_overlap)** — cheapest possible win on the new DZ path.
3. **Try 2a (pseudoangle hybrid)** — if raycasting is now dominant, this addresses the hottest path.
4. **4a (OBB caching)** — clean code improvement with potential benefit for mutation-heavy workloads.
