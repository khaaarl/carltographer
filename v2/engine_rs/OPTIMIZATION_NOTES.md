# Rust Visibility Optimization Notes

Status: **post pseudoangle bucketing** — parity test needed

## Context

The visibility system computes per-observer visibility polygons via angular-sweep raycasting, then uses those polygons for several scoring metrics:

- **Overall visibility**: Grid sample points inside the visibility polygon vs total grid points.
- **DZ hideability**: Whether the visibility polygon overlaps the opponent's expanded deployment zone (polygon-polygon intersection test).
- **Objective hidability**: Whether objective sample points fall inside the visibility polygon (PIP via Z-sorted binary search).

The angular-sweep raycasting (`compute_visibility_polygon`) casts rays toward segment endpoints (±epsilon), finds the nearest intersection per ray, and forms the visibility polygon. The hot loop is O(R × S) where R = rays, S = segments, reduced to O(R × k) by angular bucketing.

Real-world workloads: ~100-200 unique endpoints → ~300-600 rays, ~40-80 terrain segments + 4 table boundary segments.

## Benchmark Setup (IMPORTANT)

The benchmark fixtures in `benches/generate_bench.rs` were originally using **2" tall crates**, which are below the 4" `min_blocking_height` threshold. This meant the "visibility" benchmarks weren't actually testing the intersection loop at all — every observer hit the fast path (0 segments). The fixtures were fixed to use **5" tall crates** as part of this work.

Baseline numbers (with corrected 5" crates, before any optimization):
- `visibility_50`: 872 ms
- `visibility_100`: 3.80 s
- `mission_hna`: 5.70 s

### Benchmark fixtures (28-case pairwise covering array)

The benchmarks use a pairwise (all-pairs) covering array to exercise all 2-way parameter interactions with minimal test cases. Generated by `benches/generate_fixtures.py`.

**Parameters (10):**
- **Table size** (3): 44×30 (incursion), 60×44 (strike), 90×44 (onslaught)
- **Symmetry** (2): off, on (forced off when mission=none)
- **Mission** (7): none, Hammer & Anvil, Dawn of War, Tipping Point, Sweeping Engagement, Crucible of Battle, Search & Destroy
- **Terrain** (2): crates-only (5" tall), WTC mixed (crates + ruins + walls)
- **Steps** (4): 10, 20, 50, 100
- **Feature gap** (2): 0 or 5.2" (`min_feature_gap_inches`)
- **Edge gap** (2): 0 or 5.2" (`min_edge_gap_inches`)
- **All-feature gap** (2): 0 or 3" (`min_all_feature_gap_inches`)
- **All-edge gap** (2): 0 or 3" (`min_all_edge_gap_inches`)
- **Replicas** (4): 1, 2, 4, 8 (`num_replicas` for parallel tempering)

**Constraints:**
- mission=none → symmetry forced to off
- steps >= 50 OR table >= 60×44 → replicas != 8
- steps >= 100 OR table >= 90×44 → replicas != 4

All 28 cases use seed=42, visibility enabled, scoring_targets matching UI defaults (overall=30%/w1, dz_hide=70%/w5, obj_hide=50%/w5). Mission cases include precomputed `expanded_polygons` (6" DZ expansion via shapely), so the DZ hideability polygon-polygon intersection path is actually exercised.

Gap suffix `_gFEAA` encodes gap params as 4-bit flags: F=feature_gap, E=edge_gap, A=all_feature_gap, A=all_edge_gap (0=off, 1=on: 5.2" for F/E, 3" for A/A). Replica suffix `_rN` encodes num_replicas.

| # | Benchmark | Table | Sym | Mission | Terrain | Steps | Gaps | Rep |
|---|-----------|-------|-----|---------|---------|-------|------|-----|
| 01 | `90x44_crates_none_nosym_10_g0000_r1` | 90×44 | off | none | crates | 10 | 0000 | 1 |
| 02 | `44x30_wtc_none_nosym_20_g1010_r8` | 44×30 | off | none | WTC | 20 | 1010 | 8 |
| 03 | `60x44_crates_none_nosym_50_g1101_r4` | 60×44 | off | none | crates | 50 | 1101 | 4 |
| 04 | `44x30_wtc_none_nosym_100_g0111_r2` | 44×30 | off | none | WTC | 100 | 0111 | 2 |
| 05 | `44x30_wtc_HnA_nosym_10_g1001_r8` | 44×30 | off | HnA | WTC | 10 | 1001 | 8 |
| 06 | `90x44_crates_HnA_sym_20_g0100_r1` | 90×44 | on | HnA | crates | 20 | 0100 | 1 |
| 07 | `60x44_wtc_HnA_nosym_50_g0110_r4` | 60×44 | off | HnA | WTC | 50 | 0110 | 4 |
| 08 | `60x44_crates_HnA_sym_100_g1011_r2` | 60×44 | on | HnA | crates | 100 | 1011 | 2 |
| 09 | `44x30_wtc_DoW_sym_10_g0110_r8` | 44×30 | on | DoW | WTC | 10 | 0110 | 8 |
| 10 | `60x44_crates_DoW_nosym_20_g1001_r4` | 60×44 | off | DoW | crates | 20 | 1001 | 4 |
| 11 | `90x44_crates_DoW_sym_50_g0011_r2` | 90×44 | on | DoW | crates | 50 | 0011 | 2 |
| 12 | `90x44_wtc_DoW_nosym_100_g1100_r1` | 90×44 | off | DoW | WTC | 100 | 1100 | 1 |
| 13 | `60x44_wtc_TipPt_sym_10_g1010_r2` | 60×44 | on | TipPt | WTC | 10 | 1010 | 2 |
| 14 | `44x30_crates_TipPt_nosym_20_g0101_r8` | 44×30 | off | TipPt | crates | 20 | 0101 | 8 |
| 15 | `44x30_crates_TipPt_sym_50_g1001_r4` | 44×30 | on | TipPt | crates | 50 | 1001 | 4 |
| 16 | `90x44_wtc_TipPt_nosym_100_g0110_r1` | 90×44 | off | TipPt | WTC | 100 | 0110 | 1 |
| 17 | `60x44_crates_SwpEng_nosym_10_g0101_r4` | 60×44 | off | SwpEng | crates | 10 | 0101 | 4 |
| 18 | `44x30_wtc_SwpEng_sym_20_g1010_r8` | 44×30 | on | SwpEng | WTC | 20 | 1010 | 8 |
| 19 | `90x44_crates_SwpEng_nosym_50_g1101_r2` | 90×44 | off | SwpEng | crates | 50 | 1101 | 2 |
| 20 | `60x44_wtc_SwpEng_sym_100_g0010_r1` | 60×44 | on | SwpEng | WTC | 100 | 0010 | 1 |
| 21 | `44x30_crates_Crucible_sym_10_g1000_r8` | 44×30 | on | Crucible | crates | 10 | 1000 | 8 |
| 22 | `60x44_wtc_Crucible_nosym_20_g0111_r2` | 60×44 | off | Crucible | WTC | 20 | 0111 | 2 |
| 23 | `60x44_wtc_Crucible_sym_50_g0110_r4` | 60×44 | on | Crucible | WTC | 50 | 0110 | 4 |
| 24 | `90x44_crates_Crucible_nosym_100_g1001_r1` | 90×44 | off | Crucible | crates | 100 | 1001 | 1 |
| 25 | `60x44_wtc_SnD_nosym_10_g0111_r4` | 60×44 | off | SnD | WTC | 10 | 0111 | 4 |
| 26 | `44x30_crates_SnD_sym_20_g1000_r8` | 44×30 | on | SnD | crates | 20 | 1000 | 8 |
| 27 | `44x30_wtc_SnD_nosym_50_g0010_r1` | 44×30 | off | SnD | WTC | 50 | 0010 | 1 |
| 28 | `90x44_crates_SnD_sym_100_g1101_r2` | 90×44 | on | SnD | crates | 100 | 1101 | 2 |

Criterion config: `sample_size(10)`, `warm_up_time(500ms)`, `measurement_time(3s)` — total suite runtime ~7 minutes.

For quick iteration, filter to specific cases: `cargo bench -- 60x44_crates_HnA` or `cargo bench -- _r8` (all 8-replica cases).

### Baseline results (February 2026, 10-parameter suite)

| # | Benchmark | Rep | Mean |
|---|-----------|-----|------|
| 01 | `90x44_crates_none_nosym_10_g0000_r1` | 1 | 17.3 ms |
| 02 | `44x30_wtc_none_nosym_20_g1010_r8` | 8 | 46.0 ms |
| 03 | `60x44_crates_none_nosym_50_g1101_r4` | 4 | 162 ms |
| 04 | `44x30_wtc_none_nosym_100_g0111_r2` | 2 | 119 ms |
| 05 | `44x30_wtc_HnA_nosym_10_g1001_r8` | 8 | 72.9 ms |
| 06 | `90x44_crates_HnA_sym_20_g0100_r1` | 1 | 148 ms |
| 07 | `60x44_wtc_HnA_nosym_50_g0110_r4` | 4 | 407 ms |
| 08 | `60x44_crates_HnA_sym_100_g1011_r2` | 2 | 877 ms |
| 09 | `44x30_wtc_DoW_sym_10_g0110_r8` | 8 | 88.4 ms |
| 10 | `60x44_crates_DoW_nosym_20_g1001_r4` | 4 | 116 ms |
| 11 | `90x44_crates_DoW_sym_50_g0011_r2` | 2 | 646 ms |
| 12 | `90x44_wtc_DoW_nosym_100_g1100_r1` | 1 | 842 ms |
| 13 | `60x44_wtc_TipPt_sym_10_g1010_r2` | 2 | 70.8 ms |
| 14 | `44x30_crates_TipPt_nosym_20_g0101_r8` | 8 | 162 ms |
| 15 | `44x30_crates_TipPt_sym_50_g1001_r4` | 4 | 300 ms |
| 16 | `90x44_wtc_TipPt_nosym_100_g0110_r1` | 1 | 893 ms |
| 17 | `60x44_crates_SwpEng_nosym_10_g0101_r4` | 4 | 58.3 ms |
| 18 | `44x30_wtc_SwpEng_sym_20_g1010_r8` | 8 | 271 ms |
| 19 | `90x44_crates_SwpEng_nosym_50_g1101_r2` | 2 | 361 ms |
| 20 | `60x44_wtc_SwpEng_sym_100_g0010_r1` | 1 | **1.11 s** |
| 21 | `44x30_crates_Crucible_sym_10_g1000_r8` | 8 | 82.0 ms |
| 22 | `60x44_wtc_Crucible_nosym_20_g0111_r2` | 2 | 91.4 ms |
| 23 | `60x44_wtc_Crucible_sym_50_g0110_r4` | 4 | 515 ms |
| 24 | `90x44_crates_Crucible_nosym_100_g1001_r1` | 1 | 790 ms |
| 25 | `60x44_wtc_SnD_nosym_10_g0111_r4` | 4 | 69.5 ms |
| 26 | `44x30_crates_SnD_sym_20_g1000_r8` | 8 | 185 ms |
| 27 | `44x30_wtc_SnD_nosym_50_g0010_r1` | 1 | 156 ms |
| 28 | `90x44_crates_SnD_sym_100_g1101_r2` | 2 | **1.23 s** |

**Observations:**
- Replicas add significant cost: rep=8 cases on small tables (44×30, 10-20 steps) run ~3-8x slower than equivalent rep=1. Rep=2 and rep=4 scale roughly linearly.
- The heaviest cases are now the large-table high-step mission workloads: 28 (90×44 SnD sym 100 r2) at 1.23s and 20 (60×44 SwpEng sym 100 r1) at 1.11s.
- The replica exclusion constraints keep benchmark runtime manageable — 8 replicas only on small/fast workloads, 4 replicas excluded from onslaught and 100-step cases.
- Symmetry remains expensive: sym cases are ~2-3x slower than nosym at same step count.
- Gap enforcement adds ~5-15% overhead on mutation-heavy workloads.
- 10-step r1 cases remain fast (~17ms) for quick regression checks.

## Committed Optimizations

### 1. Angular bucketing (commit `47e451c`)
Partition segments by angular extent into 64 buckets. Each ray only tests segments in its bucket, reducing work from O(R×S) to O(R×k) where k ≈ average segments per bucket.

- visibility_50: 872ms → 688ms (**-21%**)
- visibility_100: 3.80s → 2.19s (**-42%**)
- mission_hna: 5.70s → 4.76s (**-16%**)

### 2. ~~Trig reduction in ray generation~~ (commit `43c1bfa`, **reverted**)
Originally replaced 6 cos/sin calls per endpoint with 1 sqrt + small-angle rotation matrix for ±epsilon rays. **Reverted for FP parity**: the rotation matrix `ndx * cos_eps ± ndz * sin_eps` produces different IEEE 754 results than Python's `cos(angle ± eps)` / `sin(angle ± eps)`, causing visibility polygon vertex differences that propagated into DZ PIP boundary tests. The code now uses `cos(angle ± eps)` / `sin(angle ± eps)` (4 trig calls per endpoint) to match Python exactly.

### 3. Inlined intersection + removed polygon buffer (commit `dfd7351`)
Inline `ray_segment_intersection` in the hot loop to enable early exit when `t >= min_t` (skips u computation). Eliminate intermediate `polygon` Vec, write directly to result.

- visibility_50: 688ms → 583ms (**-15%**)
- visibility_100: 2.19s → 1.84s (**-16%**)
- mission_hna: 4.76s → 4.88s (~same)

*Note: numbers updated to reflect opt #2 revert (baseline is post-bucketing, not post-trig-reduction).*

### 4. Rayon parallel observer loop (commit `3f719bc`)
Parallelize the observer loop in `compute_layout_visibility` using `rayon::par_iter().fold().reduce()`. Each thread gets a `Box<ThreadAccum>` with its own working buffers and accumulators. Merge via sum for ratios/counts and OR for boolean seen arrays.

Note: rayon parallelism changes the order observers are processed, but the final result is mathematically identical (addition is commutative). Parity with Python engine is maintained.

### 5–6. ~~Z-sorted binary search for DZ PIP~~ (**superseded by architecture change**)
Optimizations #5 and #6 introduced Z-sorted binary search (`DzSortedSamples`, `fraction_of_dz_visible_zsorted`, `pip_zsorted_update_seen`) for point-in-polygon testing against DZ sample grids. These were the biggest wins on mission workloads (~49% and ~41% respectively on `mission_hna`).

**These optimizations are now largely superseded.** The DZ visibility system was refactored (commit `703487f`) to replace PIP-based grid sampling with polygon-polygon intersection (`polygons_overlap`). The old `dz_visibility` and `dz_to_dz_visibility` metrics (which tested hundreds of DZ sample points against each observer's visibility polygon) were replaced by a single `dz_hideability` metric that tests whether the visibility polygon overlaps the opponent's expanded DZ polygon — a direct geometric check with no sampling.

Z-sorted PIP code still exists in the codebase and is used for **objective hidability** (`pip_zsorted_update_seen` for objective sample points), but the dominant DZ paths no longer use it.

### 7. DZ visibility: polygon-polygon intersection (commit `703487f`)
Replace expensive per-observer PIP sampling for DZ metrics with direct polygon-polygon intersection testing. DZ polygons are expanded by a fixed radius (using shapely in Python, passed to Rust as precomputed `expanded_polygons`). Each observer tests whether its visibility polygon overlaps the opponent's expanded DZ — a single `polygons_overlap()` call per DZ pair, using edge-edge intersection + vertex containment.

This eliminates the entire DZ sample grid, Z-sorted binary search for DZ PIP, and complex batch encoding logic. The new `dz_hideability` metric is both faster and more precise (exact geometric intersection vs grid approximation).

- visibility_50: ~201ms → ~191ms (no DZs — within noise)
- visibility_100: ~719ms → ~733ms (no DZs — within noise)
- **mission_hna: ~629ms → ~364ms (-42%)**
- **mission_ruins: ~371ms → ~192ms (-48%)**

### 8. AABB early-exit for polygons_overlap (precomputed DZ AABBs)
Precompute axis-aligned bounding boxes for each expanded DZ polygon (constant per layout). In the per-observer hot loop, compute the vis_poly AABB on-the-fly and reject non-overlapping pairs before the expensive O(E_A x E_B) edge-edge intersection tests.

Key design choice: a naive approach that computes *both* AABBs inside `polygons_overlap` caused a regression on some workloads (+9% on SwpEng_nosym_100) because when AABBs always overlap, the O(n_b) scan of the static DZ polygon was pure overhead. By precomputing the DZ AABB once and only computing the vis_poly AABB per call (`polygons_overlap_aabb`), the per-call cost is reduced to O(n_vis_poly) + 4 comparisons.

Impact varies by mission type and table size. Largest wins on large-table symmetric cases where many observers have visibility polygons that don't reach distant DZs:

- **90x44_wtc_HnA_sym_20**: 219 ms -> 181 ms (**-17.5%**)
- **90x44_wtc_DoW_sym_100**: 1717 ms -> 1473 ms (**-14.2%**)
- **90x44_crates_Crucible_nosym_100**: 787 ms -> 712 ms (**-9.5%**)
- **44x30_wtc_HnA_nosym_50**: 157 ms -> 144 ms (**-8.4%**)
- **60x44_crates_HnA_sym_100**: 667 ms -> 621 ms (**-7.0%**)
- Non-mission benchmarks: unaffected (no `polygons_overlap` calls)
- No regressions observed

### 9. FxHash for endpoint deduplication and objective hidability lookups
Replace the standard library `HashSet<(u64, u64)>` (SipHash-2-4, cryptographic quality) with a custom inline FxHash hasher for all integer-keyed hash sets/maps in the visibility hot paths. SipHash provides DoS resistance which is unnecessary for non-adversarial keys like `f64::to_bits()` pairs and `usize` indices. FxHash uses a single `(rotate_left(5) ^ key).wrapping_mul(MAGIC)` per 8-byte chunk -- roughly 5-10x faster per hash operation.

Affected data structures:
- `VisBuffers::endpoint_seen: FxHashSet<(u64, u64)>` -- per-observer endpoint dedup (hot path)
- `candidate_origins: FxHashSet<(u64, u64)>` -- objective hidability square candidates
- `pt_index: FxHashMap<(u64, u64), usize>` -- objective sample point lookup
- `hidden: FxHashSet<usize>` -- hidden index set for objective hidability

No external dependency added: the FxHasher is implemented inline (~25 lines) in `visibility.rs`. The change is internal-only -- produces identical output since the same set of unique endpoints is collected regardless of hash function.

Broad improvement across all workloads (5-15% typical, up to 21% on some cases):

| # | Benchmark | Before | After | Change |
|---|-----------|--------|-------|--------|
| 01 | `44x30_crates_none_nosym_10_g0000` | 10.1 ms | 9.4 ms | **-7%** |
| 02 | `60x44_wtc_none_nosym_20_g1010` | 29.2 ms | 27.8 ms | **-5%** |
| 03 | `90x44_crates_none_nosym_50_g1101` | 145 ms | 141 ms | -3% |
| 04 | `44x30_wtc_none_nosym_100_g0111` | 103 ms | 98.8 ms | **-4%** |
| 06 | `90x44_wtc_HnA_sym_20_g0100` | 187 ms | 163 ms | **-13%** |
| 08 | `60x44_crates_HnA_sym_100_g1011` | 635 ms | 574 ms | **-10%** |
| 09 | `90x44_wtc_DoW_nosym_10_g0110` | 36.1 ms | 31.8 ms | **-11%** |
| 10 | `44x30_crates_DoW_sym_20_g1001` | 38.1 ms | 34.8 ms | **-11%** |
| 11 | `60x44_crates_DoW_nosym_50_g0011` | 201 ms | 180 ms | **-10%** |
| 12 | `90x44_wtc_DoW_sym_100_g1100` | 1468 ms | 1364 ms | **-7%** |
| 14 | `60x44_crates_TipPt_nosym_20_g0101` | 58.9 ms | 54.1 ms | **-9%** |
| 16 | `44x30_wtc_TipPt_nosym_100_g0110` | 340 ms | 309 ms | **-9%** |
| 17 | `60x44_wtc_SwpEng_sym_10_g0101` | 49.2 ms | 36.3 ms | **-21%** |
| 18 | `90x44_crates_SwpEng_nosym_20_g1010` | 83.1 ms | 71.3 ms | **-15%** |
| 25 | `44x30_crates_SnD_nosym_10_g0111` | 15.4 ms | 13.2 ms | **-14%** |
| 28 | `44x30_crates_SnD_sym_100_g1101` | 212 ms | 190 ms | **-10%** |

(Selected representative cases; 25/28 benchmarks improved by 5%+ with no regressions.)

### 10. Pseudoangle for bucket assignment (replacing atan2 in bucketing)

Replace `atan2` calls in the segment bucketing loop and ray bucket lookup with a cheap pseudoangle function `pseudoangle(dx, dz) = if dz >= 0 { 3 - dx/(|dx|+|dz|) } else { 1 + dx/(|dx|+|dz|) }`. The pseudoangle maps monotonically from [0, 4) as angle goes from -pi to pi, so it preserves sort order and bucket consistency.

This eliminates 2 atan2 calls per segment per observer in the bucketing loop (lines 438-439 in the old code), replacing them with arithmetic (2 abs + 1 div + 1 comparison per endpoint). The ray sort key also switches from real angle to pseudoangle (same ordering). Ray directions (dx, dz) are completely unchanged -- the atan2 + cos/sin for epsilon rays remain for FP parity.

The pseudoangle-based buckets are non-uniform in real angle space: buckets near cardinal directions (0, pi/2, pi, -pi/2) cover wider angular ranges than buckets near diagonals. The existing +-1 bucket expansion (safety margin) absorbs this without correctness impact. The load imbalance adds a small overhead to the intersection loop, partially offsetting the atan2 savings. Overall, the net effect is positive for most workloads.

**Note**: The earlier abandoned "pseudoangle" attempt (see below) replaced atan2 everywhere including ray generation, which caused different bucket distribution AND was a more invasive change. This attempt only replaces the bucketing atan2 while keeping all ray generation trig intact.

Impact varies by workload -- largest on medium-step cases where bucketing atan2 is a proportionally larger cost:

| # | Benchmark | Before | After | Change |
|---|-----------|--------|-------|--------|
| 03 | `60x44_crates_none_nosym_50_g1101_r4` | 162 ms | 128 ms | **-21%** |
| 06 | `90x44_crates_HnA_sym_20_g0100_r1` | 148 ms | 128 ms | **-14%** |
| 07 | `60x44_wtc_HnA_nosym_50_g0110_r4` | 407 ms | 355 ms | **-13%** |
| 08 | `60x44_crates_HnA_sym_100_g1011_r2` | 877 ms | 760 ms | **-13%** |
| 11 | `90x44_crates_DoW_sym_50_g0011_r2` | 646 ms | 548 ms | **-15%** |
| 18 | `44x30_wtc_SwpEng_sym_20_g1010_r8` | 271 ms | 238 ms | **-12%** |
| 19 | `90x44_crates_SwpEng_nosym_50_g1101_r2` | 361 ms | 305 ms | **-16%** |
| 20 | `60x44_wtc_SwpEng_sym_100_g0010_r1` | 1.11 s | 976 ms | **-12%** |
| 21 | `44x30_crates_Crucible_sym_10_g1000_r8` | 82 ms | 69 ms | **-16%** |
| 23 | `60x44_wtc_Crucible_sym_50_g0110_r4` | 515 ms | 442 ms | **-14%** |
| 26 | `44x30_crates_SnD_sym_20_g1000_r8` | 185 ms | 158 ms | **-15%** |
| 27 | `44x30_wtc_SnD_nosym_50_g0010_r1` | 156 ms | 134 ms | **-14%** |
| 28 | `90x44_crates_SnD_sym_100_g1101_r2` | 1.23 s | 1.06 s | **-14%** |

(Remaining benchmarks within noise +-3%; no regressions observed.)

**IMPORTANT: Parity tests could not be run due to permission restrictions in this session. The user must run `python scripts/build_rust_engine.py` to verify parity before considering this optimization confirmed. The optimization is internal-only (bucket assignment strategy), preserves all ray directions, intersection logic, and visibility polygon output, but this has not been verified via the automated parity comparison.**

### Cumulative improvement (all optimizations)
| Benchmark | Original | Current | Total improvement |
|---|---|---|---|
| visibility_50 | 872 ms | **~191 ms** | **-78%** |
| visibility_100 | 3.80 s | **~733 ms** | **-81%** |
| mission_hna | 5.70 s | **~364 ms** | **-94%** |
| mission_ruins | n/a | **~192 ms** | n/a |

*Note: cumulative table uses old benchmark names from before the 28-case refactor. The FxHash optimization (opt #9) provides a further ~7-15% across the board on top of previous optimizations. Run-to-run variance makes precise cumulative numbers difficult to pin down, but the improvement is consistent and statistically significant across all 28 cases.*

*Measured February 2026.*

### FP parity reversions (post-optimization correctness fixes)

Three micro-optimizations were reverted because they produced different IEEE 754 floating-point results than the Python engine, breaking bit-identical parity:

1. **Trig reduction (opt #2)**: Rotation matrix for ±eps rays → reverted to `cos(angle ± eps)` / `sin(angle ± eps)`. Cost: +4 trig calls per endpoint.
2. **Precomputed inv_dz (opt #5 micro-opt)**: `1.0 / (zj - zi)` then multiply → reverted to direct division `/ (zj - zi)`. Cost: 1 division per edge per Z-range point instead of 1 multiply.
3. **Ray normalization inv_len**: `dx * (1.0 / len)` → reverted to `dx / len`. Cost: negligible (1 division vs 1 multiply, once per endpoint).

**Lesson learned**: Any arithmetic expression that produces values used in boundary tests (PIP edge crossings, visibility polygon vertices fed to DZ PIP) must use the exact same FP expression order as Python. `a * b * (1/c)` ≠ `a * b / c` and `cos(atan2(y,x) ± eps)` ≠ rotation matrix at IEEE 754 level.

## Attempted But Abandoned

### Segment-first loop reordering
Flip ray-outer/segment-inner to segment-outer/ray-inner with a `min_t[]` array (like `batch_point_in_polygon` does for PIP). The idea was to keep segment data in registers while scanning rays linearly.

**Result**: No clear improvement (±3% noise). The array-based `min_t` prevents register allocation of the per-ray minimum, and LLVM already does a good job with the original loop.

### Precomputed segment data
Precompute `(sx, sz, d_x1, d_z1, num_t)` per segment before the ray loop to avoid redundant arithmetic.

**Result**: Made things **worse** (~+15%). The 40-byte precomputed tuples increased the inner loop's memory footprint. The original 32-byte segment tuples with recomputed arithmetic were faster due to better cache behavior.

### AABB pre-filter on batch_point_in_polygon
Compute the bounding box of the visibility polygon and build a candidates list of points inside the AABB. Only run the edge-crossing loop on candidates. The idea was to skip 60-80% of DZ points when the vis polygon is small.

**Result**: No improvement on mission_hna. With sparse terrain, each observer's visibility polygon covers nearly the entire table, so the AABB filter skips almost nothing while adding overhead. The AABB filter has fundamentally wrong assumptions about the problem: in visibility analysis, most observers see most of the table.

### Pseudoangle (replacing atan2)
Replace atan2 with a cheap pseudoangle function `p = dx / (|dx| + |dz|)` that maps monotonically to [0, 4). Eliminates all atan2 calls (ray generation + bucket assignment).

**Result**: Mixed. Helped visibility_50 (-19%) and visibility_100 (-14%), but mission_hna regressed (+12%). The pseudoangle maps non-linearly to real angles, causing uneven bucket distribution. Buckets near the cardinal axes become wider (more segments), creating load imbalance. **Worth retrying with a clean machine.**

### OBB caching in is_valid_placement (4a)
Compute OBBs with height info once per feature via `get_world_obbs_with_height()` returning `Vec<(Corners, f64)>`, then reuse for overlap, all-feature-gap, tall-edge-gap, and tall-feature-gap checks. Also avoids cloning `PlacedFeature` structs into the `other_features` Vec by computing mirror OBBs directly via `get_mirror_obbs_with_height()`.

**Result**: No improvement. All 28 benchmarks within noise (+-2%). The reason: with typical layouts of 5-15 features (each having 1-3 shapes), the redundant OBB computation amounts to maybe 30-90 extra transform/trig operations per `is_valid_placement` call. This is negligible compared to the visibility computation that dominates total runtime. Additionally, `is_valid_placement` frequently early-exits on the overlap check (step 2) before reaching the gap checks (steps 2c, 3, 4) where the redundancy is worst, further limiting the potential savings. Not worth retrying -- the OBB path is simply not a bottleneck.

## Profiling Results

### Historical — pre-polygon-intersection (no longer current)

The following profiles were taken before the DZ visibility refactor and are retained for reference. The `dz_vis` and `cross_dz` phases no longer exist in their old form — they have been replaced by polygon-polygon intersection.

<details>
<summary>Post-Z-sorted-dz_vis, pre-Z-sorted-cross_dz (mission_hna)</summary>

Late-game steps (~760 observers, ~20 segments):

| Phase | Thread-ms | % of observer loop |
|---|---|---|
| `compute_visibility_polygon` | 30-50 | **~15-18%** |
| `dz_vis` (Z-sorted) | 20-30 | **~9-12%** |
| `cross_dz` (batch PIP) | 100-130 | **~45-50%** |
| `obj_hide` (batch PIP) | 55-80 | **~25-30%** |

</details>

<details>
<summary>Post-rayon, pre-Z-sorted (mission_hna)</summary>

| Phase | Thread-ms | % of observer loop |
|---|---|---|
| `get_observer_segments` | 0.2 | ~0.04% |
| `compute_visibility_polygon` | 35-48 | **~8%** |
| `dz_vis` (fraction_of_dz_visible_batch) | 270-330 | **~60%** |
| `cross_dz` (batch_point_in_polygon) | 120-148 | **~27%** |
| `obj_hide` (batch_point_in_polygon) | 30-47 | **~7%** |

</details>

### Current state: needs re-profiling (post pseudoangle + FxHash)

The polygon-polygon intersection refactor and subsequent optimizations (FxHash, pseudoangle bucketing) have significantly changed the bottleneck distribution. The old DZ PIP phases (`dz_vis`, `cross_dz`) are replaced by `polygons_overlap` calls. Bucketing atan2 has been eliminated. Fresh profiling is needed. Expected phases:

- `compute_visibility_polygon` — ray generation (atan2 + cos/sin per endpoint) + intersection loop (dominant for high-segment workloads)
- `dz_hideability` — polygon-polygon intersection per observer per opponent DZ
- `obj_hidability` — Z-sorted PIP for objective sample points (still uses old approach)
- `overall_pip` — polygon area (shoelace, likely cheap)

## Future Optimization Ideas (Not Yet Tried)

Organized by expected impact. Updated after polygon-intersection refactor.

### Tier 1: Polygon-polygon intersection optimization

The `polygons_overlap()` function (in `collision.rs`) uses edge-edge intersection + vertex containment. AABB early-exit has been added (opt #8) via `polygons_overlap_aabb()` with precomputed DZ AABBs.

#### 1b. Spatial acceleration for edge-edge tests
If expanded DZ polygons have many edges (shapely buffer can produce ~20-50 points per polygon), the O(E_vis × E_dz) edge test becomes significant. Precomputing a spatial index (e.g., grid-based or segment tree) on the static DZ polygon edges could reduce per-observer work. Likely diminishing returns now that AABB eliminates the non-overlapping cases entirely.

### Tier 2: Raycasting refinements (dominant for non-mission workloads)

With DZ PIP eliminated as a bottleneck, raycasting (`compute_visibility_polygon`) is likely the dominant cost again for mission workloads as well as non-mission. These ideas target the angular sweep hot loop.

#### ~~2a. Pseudoangle hybrid~~ (DONE — see opt #10)
Pseudoangle for bucket assignment and sort keys. 12-21% improvement on medium workloads.

#### ~~2b. Faster endpoint deduplication~~ (DONE — see opt #9)
Replaced SipHash with inline FxHash for all integer-keyed hash sets/maps. 5-21% improvement across all workloads.

#### 2c. SIMD intersection
Manually vectorize the ray-segment intersection using `std::arch` SIMD intrinsics. Complex to implement and maintain. Low priority unless profiling shows raycasting > 50%.

### Tier 3: Objective hidability PIP (still uses Z-sorted binary search)

Objective hidability still uses `pip_zsorted_update_seen` to test objective sample points against visibility polygons. With the DZ PIP paths removed, this is the only remaining PIP-heavy path.

#### 3a. Slab decomposition for objective PIP
Preprocess the visibility polygon into horizontal slabs (O(E log E) setup). Per point, binary-search to find its slab in O(log E), test against 1-2 edges. Turns PIP from O(E) per point to O(log E) per point.

With typical objective circles (~40-80 sample points per objective, 5 objectives), this is a smaller workload than the old DZ PIP. May not be worth the complexity unless profiling shows obj_hide is a significant fraction.

### Tier 4: Collision / mutation path (affects all workloads)

#### ~~4a. Redundant OBB computation in is_valid_placement~~ (ABANDONED — see "Attempted But Abandoned")
Tested: no measurable improvement. OBB path is not a bottleneck (5-15 features * 1-3 shapes, early exit on overlap).

#### 4b. Mirror feature cloning in hot paths
Mirror features for rotationally symmetric layouts are cloned into a fresh Vec on every call. Could precompute mirrors once per step, or use lazy iterator adapters.

### Tier 5: Tempering / allocation overhead

#### 5a. Pre-allocate sub_undos
`Vec::with_capacity(num_mutations)` allocated fresh for every step of every replica. Could reuse with `clear()`.

#### 5b. Layout cloning on swap and best-tracking
Full `TerrainLayout` cloned on replica swaps. Could use COW semantics.

### Tier 6: Algorithmic / architectural (high complexity, speculative)

#### 6a. Incremental visibility
Only recompute observers affected by a mutation. Extremely complex, especially with DZ/objective accumulation. Risk of subtle correctness bugs.

### Recommended next steps

1. **Re-profile** to see the new bottleneck distribution post-FxHash. The endpoint dedup is now much faster, so `compute_visibility_polygon`'s raycasting inner loop and atan2/trig calls are likely more dominant.
2. **Try 2a (pseudoangle hybrid)** — if raycasting is now dominant, replacing atan2 for sort-key computation addresses the hottest remaining path. Use pseudoangle for sort keys only, keep atan2 for bucket assignment (uniform distribution).
3. **Also consider**: FxHash could be applied to `objects_by_id: HashMap<String, ...>` (String keys), though this is built once per layout and unlikely to be a bottleneck.

**Note**: 4a (OBB caching) was tested and found to be negligible -- the collision/mutation path is not a bottleneck. Remaining optimization potential is primarily in the visibility system (raycasting, polygon tests).
